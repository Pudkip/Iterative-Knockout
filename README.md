# *Iterative Knockout - A Method for Finding Feature Importance and High-Order interactions in (Recurrent) Neural Networks [Python] [Keras]*



## Description
------


This repo contains python scripts for determining feature importance of a recurrent neural network in Keras. The idea is to train a a model and then selectively nullify features and recalculate accuracy. If the accuracy changes significantly, then that feature(s) is considered to be important. These scripts contain functions to calculate accuarcy for by nullifying features 1 at a time, by a determined gram size - that is n-sized chunks of features next to each other in sequence-, or by high order- feature nullification for all combinations of features. These functions will work for data formatted for standard timestep processing i.e. (sample size, # timesteps, # feature categories). Later, I may add this method for Convnets to find feature importance in images, but I think that will be too computationally demanding.


## Pipline
------


As stated earlier, the data should be in the format (n, # timesteps, # feature categories) e.g. if you were modelling text it would be (# passages, length of passages, one-hot of the word(s) that appears at that timestep). The Data script contains some functions for generating random data in this format. The model script currently builds a simple bi-directional LSTM with binary  output, so if you use this model your labels will need to be binary and in the format (n,).

There are two methods for knocking out data, one is to knockout out data and retrain the model after each knockout and compare model accuracy. Because this requires retraining for each knockout, it is not feasible for high-order interactions as a sample with 10 timesteps will require over 1000 iterations. Instead, I propose that you split your data (use sklearn.cross_validation or some shit) into two numpy arrays. Use one of the arrays to train the model. 

After the model has been trained, use the second part for the knockout process. First, use the trained model to generate predictions. Use Mean_Log_Loss from the metrics script on the predictions to calculate accuracy of the model. Then (also n the metrics script), use your desired feature knockout- Single_Iterative_Knockout, N-Gram_Iterative_Knockout, or High_Order_Iterative_Knockout. This will perform the knockout process accordingly and generate predictions for each iteration. Then it will calculate change in accuracy using the same cost function (mean log loss) from before and return a list of accuracies and a list of indices (higher number means that the iteration had a more significant effect on the model's accuracy).


## Usage
------


### Metrics

**Mean_Log_Loss(predictions, labels, limit=10)**
* **Description**: calculates accuracy of predictions using mean log loss for cost.
* **predictions**: A list of predictions as outputted from the model.
* **labels**: An array of labels that correspond, sequentially to the predictions.
* **limit**: An integer 10^-limit gets added to difference between predictions and labels to avoid taking a log of 0 which happens when keras predicts too similar to the label.

**Single_Iterative_Knockout(features_knockout, model, labels, baseline)**
* **Description**: Calculates feature importance, one feature at a time.
* **features_knockout**: An array of features that have been set aside to perform the knockout on.
* **model**: A trained Keras model for predicting features_knockout.
* **labels**: An array of labels to correspond with features_knockout.
* **baseline**: A float outputted from Mean_Log_Loss.

**N_Gram_Iterative_Knockout(features_knockout, model, labels, baseline, gram_size=2)**
* **Description**: Calculates feature importance in sequential chunks of features of a determined length.
* **features_knockout**: An array of features that have been set aside to perform the knockout on.
* **model**: A trained Keras model for predicting features_knockout.
* **labels**: An array of labels to correspond with features_knockout.
* **baseline**: A float outputted from Mean_Log_Loss.
* **gram_size**: number of features to group together.
    
**High_Order_Iterative_Knockout(features_knockout, model, labels, baseline,)**
* **Description**: Calculates feature importance for all possible combinations of features.
* **features_knockout**: An array of features that have been set aside to perform the knockout on.
* **model**: A trained Keras model for predicting features_knockout.
* **labels**: An array of labels to correspond with features_knockout.
* **baseline**: A float outputted from Mean_Log_Loss.

### Data

**Create_Features(sample_size, sequence_length, feature_length)**
* **Description**: Creates random features based on parameters.
* **sample_size**: Desired sample size.
* **sequence_length**: Desired sequence length.
* **feature_length**: Desired feature length.

**Create_Labels(sample_size)**
* **Description**: Creates random, binary labels.
* **sample_size**: Desired sample size.

**Sample_Data(sample_size,sequence_length, target)**
* **Description**: A function to generate data that emulates DNA and targets a specified site for association with a positive label.
* **sample_size**: Desired sample size.
* **sequence_length**: Desired sequence length.
* **target**: An integer of a timestep to be highly associated with a positive label.

### Models

**Create_RNN(input_shape)**
* **Description**: Creates a simple bidirectional LSTM.
* **input_shape**: a tuple with a length of 3 specifying input shape.


## Example
------


Here is an example of how to perform the iterative knockout using some sample data that I made to emulate DNA. The features have n samples, x sequence lengths, and 4 possible nucleotides (A, G, C, U). The labels are binary, think of it as having a disease (0) or not having a disease (1). In this example I will target a particular site that if a "G" appears at that site, the person will automatically be labelled as a 1. Conversely, if the sequence does not contain a "G" at the target site, it will be labelled a 0. Thus the target site should be highly important to the accuracy of the model.


```python
from Data import Sample_Data
from Models import Create_RNN
from keras.callbacks import EarlyStopping
from Metrics import Mean_Log_Loss, Single_Iterative_Knockout, N_Gram_Iterative_Knockout, High_Order_Iterative_Knockout
import numpy as np


# Set some parameters - index 5 (the 6th feature) will be highly associated with a positive label.
sample_size = 1000
sequence_length = 10
target = 5

# Generate data that will be used to train the model.
features, phenotype = Sample_Data(sample_size = sample_size, sequence_length = sequence_length, target = target)
print("features shape: ", features.shape)
print("labels shape: ", phenotype.shape)

# Generate some seperate data that will be used in predictions.
features_test, phenotype_test = Sample_Data(sample_size = sample_size, sequence_length = sequence_length, target = target)

# Train the model
model = Create_RNN(features.shape[1:])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics =['accuracy'])
model.summary()
model.fit(features, phenotype,
                 batch_size=32,
                 validation_split=0.2,
                 epochs=50,
                 callbacks=[EarlyStopping(patience=4, monitor='val_loss')],
                 verbose=0)

# Get a baseline accuracy for the model
predictions = model.predict(features_test).reshape(sample_size,).tolist()
accuracy = Mean_Log_Loss(predictions = predictions, labels = phenotype_test)
print("baseline accuracy: ",accuracy,"\n")
```

    Using TensorFlow backend.
    C:\Users\James\AppData\Local\Programs\Python\Python36\lib\site-packages\h5py\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
      from ._conv import register_converters as _register_converters
    

    features shape:  (1000, 10, 4)
    labels shape:  (1000,)
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    bidirectional_1 (Bidirection (None, 10, 256)           136192    
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 10, 256)           0         
    _________________________________________________________________
    bidirectional_2 (Bidirection (None, 10, 512)           1050624   
    _________________________________________________________________
    dropout_2 (Dropout)          (None, 10, 512)           0         
    _________________________________________________________________
    bidirectional_3 (Bidirection (None, 10, 512)           1574912   
    _________________________________________________________________
    dropout_3 (Dropout)          (None, 10, 512)           0         
    _________________________________________________________________
    bidirectional_4 (Bidirection (None, 10, 256)           656384    
    _________________________________________________________________
    global_max_pooling1d_1 (Glob (None, 256)               0         
    _________________________________________________________________
    dropout_4 (Dropout)          (None, 256)               0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 64)                16448     
    _________________________________________________________________
    dropout_5 (Dropout)          (None, 64)                0         
    _________________________________________________________________
    dense_2 (Dense)              (None, 32)                2080      
    _________________________________________________________________
    dropout_6 (Dropout)          (None, 32)                0         
    _________________________________________________________________
    dense_3 (Dense)              (None, 8)                 264       
    _________________________________________________________________
    dropout_7 (Dropout)          (None, 8)                 0         
    _________________________________________________________________
    dense_4 (Dense)              (None, 1)                 9         
    =================================================================
    Total params: 3,436,913
    Trainable params: 3,436,913
    Non-trainable params: 0
    _________________________________________________________________
    baseline accuracy:  9.527926866054859 
    
    


```python
# Generate single knockout predictions (the 5th index should be significantly higher than others).

single_iterative_knockout = Single_Iterative_Knockout(features_knockout = features_test, model = model, baseline = accuracy, labels = phenotype_test)
print("single iterative knockout accuracy change: ")
print(single_iterative_knockout)
```

    single iterative knockout accuracy change: 
    [0.035108970935823436, 0.03437247355481432, 0.0013010734957124015, 0.0008233878212315915, 0.1768184358479452, 3.798454558710884, 0.014289971848889138, 0.0628687005193882, 0.0910105793758742, 0.06518626163800967]
    

Notice that the target feature, the 6th one is about 3 orders of magnitude higher than the others.


```python
# Generate knockout predictions for a specified gram size - I'll use 3.

n_gram_knockout, index = N_Gram_Iterative_Knockout(features_knockout = features_test, model = model, baseline = accuracy, labels = phenotype_test, gram_size = 3)
print("n-gram knockout accuracy change: ")
print( n_gram_knockout,"\n")
print("index: ")
print(index)
```

    n-gram knockout accuracy change: 
    [0.04639988736460943, 0.027510534638317097, 0.20111947926918639, 2.9743778062897963, 4.689957969618205, 5.048124117883249, 0.31644427887241555] 
    
    index: 
    ['0:2', '1:3', '2:4', '3:5', '4:6', '5:7', '6:8']
    

In the n-gram knockout, iterations knocking out the target feature- '3:5', '4:6', and '5:7'- are about 1 order of magnitude higher than the others. Though, in this case, increasing gram size will decrease decrease the significance of containing the target feature as there is no association between other features and a positive phenotype in my sample data.


```python
# Generate high-order iterative knockouts

high_order_knockout, index = High_Order_Iterative_Knockout(features_knockout = features_test, model = model, baseline = accuracy, labels = phenotype_test)
print("high-order knockout accuracy change: ")
print( high_order_knockout,"\n")
print("index: ")
print(index)
```

    high-order knockout accuracy change: 
    [0.04639988736460943, 0.04639988736460943, 0.04639988736460943, 0.027510534638317097, 0.19488173720568724, 3.374308469595647, 0.052744375314214054, 0.021825068367524736, 0.08098510601248954, 0.037281014078338615, 0.04639988736460943, 0.04639988736460943, 0.027510534638317097, 0.19488173720568724, 3.374308469595647, 0.052744375314214054, 0.021825068367524736, 0.08098510601248954, 0.037281014078338615, 0.04639988736460943, 0.027510534638317097, 0.19488173720568724, 3.374308469595647, 0.052744375314214054, 0.021825068367524736, 0.08098510601248954, 0.037281014078338615, 0.027510534638317097, 0.19488173720568724, 3.374308469595647, 0.052744375314214054, 0.021825068367524736, 0.08098510601248954, 0.037281014078338615, 0.1843035110784097, 2.8088940414296495, 0.07287416234685118, 0.04626442049646151, 0.11138933160063935, 0.10936076886751422, 3.2698276957628627, 0.21968805385313317, 0.3666712373227057, 0.22230182448790003, 0.1713302152511229, 4.734180187251919, 3.587362501682242, 3.8294607064979944, 3.1624288135366916, 0.058666419199772335, 0.2443440964969703, 0.20717623728584655, 0.12392009172628704, 0.09646807570000782, 0.19132259936976403, 0.04639988736460943, 0.027510534638317097, 0.19488173720568724, 3.374308469595647, 0.052744375314214054, 0.021825068367524736, 0.08098510601248954, 0.037281014078338615, 0.027510534638317097, 0.19488173720568724, 3.374308469595647, 0.052744375314214054, 0.021825068367524736, 0.08098510601248954, 0.037281014078338615, 0.1843035110784097, 2.8088940414296495, 0.07287416234685118, 0.04626442049646151, 0.11138933160063935, 0.10936076886751422, 3.2698276957628627, 0.21968805385313317, 0.3666712373227057, 0.22230182448790003, 0.1713302152511229, 4.734180187251919, 3.587362501682242, 3.8294607064979944, 3.1624288135366916, 0.058666419199772335, 0.2443440964969703, 0.20717623728584655, 0.12392009172628704, 0.09646807570000782, 0.19132259936976403, 0.027510534638317097, 0.19488173720568724, 3.374308469595647, 0.052744375314214054, 0.021825068367524736, 0.08098510601248954, 0.037281014078338615, 0.1843035110784097, 2.8088940414296495, 0.07287416234685118, 0.04626442049646151, 0.11138933160063935, 0.10936076886751422, 3.2698276957628627, 0.21968805385313317, 0.3666712373227057, 0.22230182448790003, 0.1713302152511229, 4.734180187251919, 3.587362501682242, 3.8294607064979944, 3.1624288135366916, 0.058666419199772335, 0.2443440964969703, 0.20717623728584655, 0.12392009172628704, 0.09646807570000782, 0.19132259936976403, 0.1843035110784097, 2.8088940414296495, 0.07287416234685118, 0.04626442049646151, 0.11138933160063935, 0.10936076886751422, 3.2698276957628627, 0.21968805385313317, 0.3666712373227057, 0.22230182448790003, 0.1713302152511229, 4.734180187251919, 3.587362501682242, 3.8294607064979944, 3.1624288135366916, 0.058666419199772335, 0.2443440964969703, 0.20717623728584655, 0.12392009172628704, 0.09646807570000782, 0.19132259936976403, 2.8547430207062945, 0.22634134145111062, 0.4258830698531426, 0.17796891004468307, 0.12885996909805364, 4.139879116486767, 3.101583556579527, 3.192691031955686, 2.7153131343598096, 0.10326163841709324, 0.21587628772912737, 0.23772187707904102, 0.20051989744865395, 0.19039203963900242, 0.2543550890908328, 4.589429408365294, 3.7375721227714562, 3.8310065575323113, 3.0131735230689243, 0.47874715854687366, 0.1147516393373742, 0.3312577684624962, 0.42477877108902184, 0.3489568684626274, 0.18006912643432926, 5.08718343631219, 5.223274698710005, 4.294880209017686, 4.138878833943083, 3.412256736956966, 3.4387810790804547, 0.23199265140118186, 0.2389331534427157, 0.31208246678238183, 0.37878847948599415, 0.027510534638317097, 0.19488173720568724, 3.374308469595647, 0.052744375314214054, 0.021825068367524736, 0.08098510601248954, 0.037281014078338615, 0.1843035110784097, 2.8088940414296495, 0.07287416234685118, 0.04626442049646151, 0.11138933160063935, 0.10936076886751422, 3.2698276957628627, 0.21968805385313317, 0.3666712373227057, 0.22230182448790003, 0.1713302152511229, 4.734180187251919, 3.587362501682242, 3.8294607064979944, 3.1624288135366916, 0.058666419199772335, 0.2443440964969703, 0.20717623728584655, 0.12392009172628704, 0.09646807570000782, 0.19132259936976403, 0.1843035110784097, 2.8088940414296495, 0.07287416234685118, 0.04626442049646151, 0.11138933160063935, 0.10936076886751422, 3.2698276957628627, 0.21968805385313317, 0.3666712373227057, 0.22230182448790003, 0.1713302152511229, 4.734180187251919, 3.587362501682242, 3.8294607064979944, 3.1624288135366916, 0.058666419199772335, 0.2443440964969703, 0.20717623728584655, 0.12392009172628704, 0.09646807570000782, 0.19132259936976403, 2.8547430207062945, 0.22634134145111062, 0.4258830698531426, 0.17796891004468307, 0.12885996909805364, 4.139879116486767, 3.101583556579527, 3.192691031955686, 2.7153131343598096, 0.10326163841709324, 0.21587628772912737, 0.23772187707904102, 0.20051989744865395, 0.19039203963900242, 0.2543550890908328, 4.589429408365294, 3.7375721227714562, 3.8310065575323113, 3.0131735230689243, 0.47874715854687366, 0.1147516393373742, 0.3312577684624962, 0.42477877108902184, 0.3489568684626274, 0.18006912643432926, 5.08718343631219, 5.223274698710005, 4.294880209017686, 4.138878833943083, 3.412256736956966, 3.4387810790804547, 0.23199265140118186, 0.2389331534427157, 0.31208246678238183, 0.37878847948599415, 0.1843035110784097, 2.8088940414296495, 0.07287416234685118, 0.04626442049646151, 0.11138933160063935, 0.10936076886751422, 3.2698276957628627, 0.21968805385313317, 0.3666712373227057, 0.22230182448790003, 0.1713302152511229, 4.734180187251919, 3.587362501682242, 3.8294607064979944, 3.1624288135366916, 0.058666419199772335, 0.2443440964969703, 0.20717623728584655, 0.12392009172628704, 0.09646807570000782, 0.19132259936976403, 2.8547430207062945, 0.22634134145111062, 0.4258830698531426, 0.17796891004468307, 0.12885996909805364, 4.139879116486767, 3.101583556579527, 3.192691031955686, 2.7153131343598096, 0.10326163841709324, 0.21587628772912737, 0.23772187707904102, 0.20051989744865395, 0.19039203963900242, 0.2543550890908328, 4.589429408365294, 3.7375721227714562, 3.8310065575323113, 3.0131735230689243, 0.47874715854687366, 0.1147516393373742, 0.3312577684624962, 0.42477877108902184, 0.3489568684626274, 0.18006912643432926, 5.08718343631219, 5.223274698710005, 4.294880209017686, 4.138878833943083, 3.412256736956966, 3.4387810790804547, 0.23199265140118186, 0.2389331534427157, 0.31208246678238183, 0.37878847948599415, 2.8547430207062945, 0.22634134145111062, 0.4258830698531426, 0.17796891004468307, 0.12885996909805364, 4.139879116486767, 3.101583556579527, 3.192691031955686, 2.7153131343598096, 0.10326163841709324, 0.21587628772912737, 0.23772187707904102, 0.20051989744865395, 0.19039203963900242, 0.2543550890908328, 4.589429408365294, 3.7375721227714562, 3.8310065575323113, 3.0131735230689243, 0.47874715854687366, 0.1147516393373742, 0.3312577684624962, 0.42477877108902184, 0.3489568684626274, 0.18006912643432926, 5.08718343631219, 5.223274698710005, 4.294880209017686, 4.138878833943083, 3.412256736956966, 3.4387810790804547, 0.23199265140118186, 0.2389331534427157, 0.31208246678238183, 0.37878847948599415, 4.011326184333655, 3.3117475583521054, 3.339840921294705, 2.655441854777508, 0.48810310907683174, 0.15497940659520104, 0.3360603418561823, 0.42916747713236525, 0.32766921702108753, 0.05705086328592124, 4.558700492244825, 4.669377151267385, 3.8570187767339146, 3.5944074116594242, 2.912443437564617, 2.9390324634322456, 0.14171135538763302, 0.28671645392682876, 0.2958867779738412, 0.6366051749635133, 5.179024168056652, 5.07305140439398, 4.261627682482974, 4.422894010316348, 3.446866642270577, 3.5197406973039413, 0.21811468689800861, 0.44897167215508205, 0.3011154605226807, 0.3034038331691189, 5.426047095767712, 4.434496848332272, 4.570317689284098, 3.8020605047401324, 0.3434449264755397, 0.1843035110784097, 2.8088940414296495, 0.07287416234685118, 0.04626442049646151, 0.11138933160063935, 0.10936076886751422, 3.2698276957628627, 0.21968805385313317, 0.3666712373227057, 0.22230182448790003, 0.1713302152511229, 4.734180187251919, 3.587362501682242, 3.8294607064979944, 3.1624288135366916, 0.058666419199772335, 0.2443440964969703, 0.20717623728584655, 0.12392009172628704, 0.09646807570000782, 0.19132259936976403, 2.8547430207062945, 0.22634134145111062, 0.4258830698531426, 0.17796891004468307, 0.12885996909805364, 4.139879116486767, 3.101583556579527, 3.192691031955686, 2.7153131343598096, 0.10326163841709324, 0.21587628772912737, 0.23772187707904102, 0.20051989744865395, 0.19039203963900242, 0.2543550890908328, 4.589429408365294, 3.7375721227714562, 3.8310065575323113, 3.0131735230689243, 0.47874715854687366, 0.1147516393373742, 0.3312577684624962, 0.42477877108902184, 0.3489568684626274, 0.18006912643432926, 5.08718343631219, 5.223274698710005, 4.294880209017686, 4.138878833943083, 3.412256736956966, 3.4387810790804547, 0.23199265140118186, 0.2389331534427157, 0.31208246678238183, 0.37878847948599415, 2.8547430207062945, 0.22634134145111062, 0.4258830698531426, 0.17796891004468307, 0.12885996909805364, 4.139879116486767, 3.101583556579527, 3.192691031955686, 2.7153131343598096, 0.10326163841709324, 0.21587628772912737, 0.23772187707904102, 0.20051989744865395, 0.19039203963900242, 0.2543550890908328, 4.589429408365294, 3.7375721227714562, 3.8310065575323113, 3.0131735230689243, 0.47874715854687366, 0.1147516393373742, 0.3312577684624962, 0.42477877108902184, 0.3489568684626274, 0.18006912643432926, 5.08718343631219, 5.223274698710005, 4.294880209017686, 4.138878833943083, 3.412256736956966, 3.4387810790804547, 0.23199265140118186, 0.2389331534427157, 0.31208246678238183, 0.37878847948599415, 4.011326184333655, 3.3117475583521054, 3.339840921294705, 2.655441854777508, 0.48810310907683174, 0.15497940659520104, 0.3360603418561823, 0.42916747713236525, 0.32766921702108753, 0.05705086328592124, 4.558700492244825, 4.669377151267385, 3.8570187767339146, 3.5944074116594242, 2.912443437564617, 2.9390324634322456, 0.14171135538763302, 0.28671645392682876, 0.2958867779738412, 0.6366051749635133, 5.179024168056652, 5.07305140439398, 4.261627682482974, 4.422894010316348, 3.446866642270577, 3.5197406973039413, 0.21811468689800861, 0.44897167215508205, 0.3011154605226807, 0.3034038331691189, 5.426047095767712, 4.434496848332272, 4.570317689284098, 3.8020605047401324, 0.3434449264755397, 2.8547430207062945, 0.22634134145111062, 0.4258830698531426, 0.17796891004468307, 0.12885996909805364, 4.139879116486767, 3.101583556579527, 3.192691031955686, 2.7153131343598096, 0.10326163841709324, 0.21587628772912737, 0.23772187707904102, 0.20051989744865395, 0.19039203963900242, 0.2543550890908328, 4.589429408365294, 3.7375721227714562, 3.8310065575323113, 3.0131735230689243, 0.47874715854687366, 0.1147516393373742, 0.3312577684624962, 0.42477877108902184, 0.3489568684626274, 0.18006912643432926, 5.08718343631219, 5.223274698710005, 4.294880209017686, 4.138878833943083, 3.412256736956966, 3.4387810790804547, 0.23199265140118186, 0.2389331534427157, 0.31208246678238183, 0.37878847948599415, 4.011326184333655, 3.3117475583521054, 3.339840921294705, 2.655441854777508, 0.48810310907683174, 0.15497940659520104, 0.3360603418561823, 0.42916747713236525, 0.32766921702108753, 0.05705086328592124, 4.558700492244825, 4.669377151267385, 3.8570187767339146, 3.5944074116594242, 2.912443437564617, 2.9390324634322456, 0.14171135538763302, 0.28671645392682876, 0.2958867779738412, 0.6366051749635133, 5.179024168056652, 5.07305140439398, 4.261627682482974, 4.422894010316348, 3.446866642270577, 3.5197406973039413, 0.21811468689800861, 0.44897167215508205, 0.3011154605226807, 0.3034038331691189, 5.426047095767712, 4.434496848332272, 4.570317689284098, 3.8020605047401324, 0.3434449264755397, 4.011326184333655, 3.3117475583521054, 3.339840921294705, 2.655441854777508, 0.48810310907683174, 0.15497940659520104, 0.3360603418561823, 0.42916747713236525, 0.32766921702108753, 0.05705086328592124, 4.558700492244825, 4.669377151267385, 3.8570187767339146, 3.5944074116594242, 2.912443437564617, 2.9390324634322456, 0.14171135538763302, 0.28671645392682876, 0.2958867779738412, 0.6366051749635133, 5.179024168056652, 5.07305140439398, 4.261627682482974, 4.422894010316348, 3.446866642270577, 3.5197406973039413, 0.21811468689800861, 0.44897167215508205, 0.3011154605226807, 0.3034038331691189, 5.426047095767712, 4.434496848332272, 4.570317689284098, 3.8020605047401324, 0.3434449264755397, 4.595816747894877, 4.346907651406751, 3.7668426144412477, 3.9010662657712603, 3.006074844714525, 3.083025112948973, 0.2622363171505402, 0.5417426421013278, 0.42989756112238986, 0.2147564552259169, 4.963530222873048, 4.0619534732783436, 4.150392074036729, 3.1149282590369136, 0.6926671256785912, 5.613381572963104, 4.720526925571363, 4.613128077309473, 3.956539323891146, 0.44631103406578276, 4.509252502431731, 2.8547430207062945, 0.22634134145111062, 0.4258830698531426, 0.17796891004468307, 0.12885996909805364, 4.139879116486767, 3.101583556579527, 3.192691031955686, 2.7153131343598096, 0.10326163841709324, 0.21587628772912737, 0.23772187707904102, 0.20051989744865395, 0.19039203963900242, 0.2543550890908328, 4.589429408365294, 3.7375721227714562, 3.8310065575323113, 3.0131735230689243, 0.47874715854687366, 0.1147516393373742, 0.3312577684624962, 0.42477877108902184, 0.3489568684626274, 0.18006912643432926, 5.08718343631219, 5.223274698710005, 4.294880209017686, 4.138878833943083, 3.412256736956966, 3.4387810790804547, 0.23199265140118186, 0.2389331534427157, 0.31208246678238183, 0.37878847948599415, 4.011326184333655, 3.3117475583521054, 3.339840921294705, 2.655441854777508, 0.48810310907683174, 0.15497940659520104, 0.3360603418561823, 0.42916747713236525, 0.32766921702108753, 0.05705086328592124, 4.558700492244825, 4.669377151267385, 3.8570187767339146, 3.5944074116594242, 2.912443437564617, 2.9390324634322456, 0.14171135538763302, 0.28671645392682876, 0.2958867779738412, 0.6366051749635133, 5.179024168056652, 5.07305140439398, 4.261627682482974, 4.422894010316348, 3.446866642270577, 3.5197406973039413, 0.21811468689800861, 0.44897167215508205, 0.3011154605226807, 0.3034038331691189, 5.426047095767712, 4.434496848332272, 4.570317689284098, 3.8020605047401324, 0.3434449264755397, 4.011326184333655, 3.3117475583521054, 3.339840921294705, 2.655441854777508, 0.48810310907683174, 0.15497940659520104, 0.3360603418561823, 0.42916747713236525, 0.32766921702108753, 0.05705086328592124, 4.558700492244825, 4.669377151267385, 3.8570187767339146, 3.5944074116594242, 2.912443437564617, 2.9390324634322456, 0.14171135538763302, 0.28671645392682876, 0.2958867779738412, 0.6366051749635133, 5.179024168056652, 5.07305140439398, 4.261627682482974, 4.422894010316348, 3.446866642270577, 3.5197406973039413, 0.21811468689800861, 0.44897167215508205, 0.3011154605226807, 0.3034038331691189, 5.426047095767712, 4.434496848332272, 4.570317689284098, 3.8020605047401324, 0.3434449264755397, 4.595816747894877, 4.346907651406751, 3.7668426144412477, 3.9010662657712603, 3.006074844714525, 3.083025112948973, 0.2622363171505402, 0.5417426421013278, 0.42989756112238986, 0.2147564552259169, 4.963530222873048, 4.0619534732783436, 4.150392074036729, 3.1149282590369136, 0.6926671256785912, 5.613381572963104, 4.720526925571363, 4.613128077309473, 3.956539323891146, 0.44631103406578276, 4.509252502431731, 4.011326184333655, 3.3117475583521054, 3.339840921294705, 2.655441854777508, 0.48810310907683174, 0.15497940659520104, 0.3360603418561823, 0.42916747713236525, 0.32766921702108753, 0.05705086328592124, 4.558700492244825, 4.669377151267385, 3.8570187767339146, 3.5944074116594242, 2.912443437564617, 2.9390324634322456, 0.14171135538763302, 0.28671645392682876, 0.2958867779738412, 0.6366051749635133, 5.179024168056652, 5.07305140439398, 4.261627682482974, 4.422894010316348, 3.446866642270577, 3.5197406973039413, 0.21811468689800861, 0.44897167215508205, 0.3011154605226807, 0.3034038331691189, 5.426047095767712, 4.434496848332272, 4.570317689284098, 3.8020605047401324, 0.3434449264755397, 4.595816747894877, 4.346907651406751, 3.7668426144412477, 3.9010662657712603, 3.006074844714525, 3.083025112948973, 0.2622363171505402, 0.5417426421013278, 0.42989756112238986, 0.2147564552259169, 4.963530222873048, 4.0619534732783436, 4.150392074036729, 3.1149282590369136, 0.6926671256785912, 5.613381572963104, 4.720526925571363, 4.613128077309473, 3.956539323891146, 0.44631103406578276, 4.509252502431731, 4.595816747894877, 4.346907651406751, 3.7668426144412477, 3.9010662657712603, 3.006074844714525, 3.083025112948973, 0.2622363171505402, 0.5417426421013278, 0.42989756112238986, 0.2147564552259169, 4.963530222873048, 4.0619534732783436, 4.150392074036729, 3.1149282590369136, 0.6926671256785912, 5.613381572963104, 4.720526925571363, 4.613128077309473, 3.956539323891146, 0.44631103406578276, 4.509252502431731, 5.0122864081563305, 4.238077980583448, 4.068178043485734, 3.474812298397164, 0.01389200829499515, 4.16032057025792, 4.911473800911488, 4.011326184333655, 3.3117475583521054, 3.339840921294705, 2.655441854777508, 0.48810310907683174, 0.15497940659520104, 0.3360603418561823, 0.42916747713236525, 0.32766921702108753, 0.05705086328592124, 4.558700492244825, 4.669377151267385, 3.8570187767339146, 3.5944074116594242, 2.912443437564617, 2.9390324634322456, 0.14171135538763302, 0.28671645392682876, 0.2958867779738412, 0.6366051749635133, 5.179024168056652, 5.07305140439398, 4.261627682482974, 4.422894010316348, 3.446866642270577, 3.5197406973039413, 0.21811468689800861, 0.44897167215508205, 0.3011154605226807, 0.3034038331691189, 5.426047095767712, 4.434496848332272, 4.570317689284098, 3.8020605047401324, 0.3434449264755397, 4.595816747894877, 4.346907651406751, 3.7668426144412477, 3.9010662657712603, 3.006074844714525, 3.083025112948973, 0.2622363171505402, 0.5417426421013278, 0.42989756112238986, 0.2147564552259169, 4.963530222873048, 4.0619534732783436, 4.150392074036729, 3.1149282590369136, 0.6926671256785912, 5.613381572963104, 4.720526925571363, 4.613128077309473, 3.956539323891146, 0.44631103406578276, 4.509252502431731, 4.595816747894877, 4.346907651406751, 3.7668426144412477, 3.9010662657712603, 3.006074844714525, 3.083025112948973, 0.2622363171505402, 0.5417426421013278, 0.42989756112238986, 0.2147564552259169, 4.963530222873048, 4.0619534732783436, 4.150392074036729, 3.1149282590369136, 0.6926671256785912, 5.613381572963104, 4.720526925571363, 4.613128077309473, 3.956539323891146, 0.44631103406578276, 4.509252502431731, 5.0122864081563305, 4.238077980583448, 4.068178043485734, 3.474812298397164, 0.01389200829499515, 4.16032057025792, 4.911473800911488, 4.595816747894877, 4.346907651406751, 3.7668426144412477, 3.9010662657712603, 3.006074844714525, 3.083025112948973, 0.2622363171505402, 0.5417426421013278, 0.42989756112238986, 0.2147564552259169, 4.963530222873048, 4.0619534732783436, 4.150392074036729, 3.1149282590369136, 0.6926671256785912, 5.613381572963104, 4.720526925571363, 4.613128077309473, 3.956539323891146, 0.44631103406578276, 4.509252502431731, 5.0122864081563305, 4.238077980583448, 4.068178043485734, 3.474812298397164, 0.01389200829499515, 4.16032057025792, 4.911473800911488, 5.0122864081563305, 4.238077980583448, 4.068178043485734, 3.474812298397164, 0.01389200829499515, 4.16032057025792, 4.911473800911488, 4.479078899831331, 4.595816747894877, 4.346907651406751, 3.7668426144412477, 3.9010662657712603, 3.006074844714525, 3.083025112948973, 0.2622363171505402, 0.5417426421013278, 0.42989756112238986, 0.2147564552259169, 4.963530222873048, 4.0619534732783436, 4.150392074036729, 3.1149282590369136, 0.6926671256785912, 5.613381572963104, 4.720526925571363, 4.613128077309473, 3.956539323891146, 0.44631103406578276, 4.509252502431731, 5.0122864081563305, 4.238077980583448, 4.068178043485734, 3.474812298397164, 0.01389200829499515, 4.16032057025792, 4.911473800911488, 5.0122864081563305, 4.238077980583448, 4.068178043485734, 3.474812298397164, 0.01389200829499515, 4.16032057025792, 4.911473800911488, 4.479078899831331, 5.0122864081563305, 4.238077980583448, 4.068178043485734, 3.474812298397164, 0.01389200829499515, 4.16032057025792, 4.911473800911488, 4.479078899831331, 4.479078899831331, 5.0122864081563305, 4.238077980583448, 4.068178043485734, 3.474812298397164, 0.01389200829499515, 4.16032057025792, 4.911473800911488, 4.479078899831331, 4.479078899831331, 4.479078899831331] 
    
    index: 
    [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [0, 9], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [5, 6], [5, 7], [5, 8], [5, 9], [6, 7], [6, 8], [6, 9], [7, 8], [7, 9], [8, 9], [0, 1, 2], [0, 1, 3], [0, 1, 4], [0, 1, 5], [0, 1, 6], [0, 1, 7], [0, 1, 8], [0, 1, 9], [0, 2, 3], [0, 2, 4], [0, 2, 5], [0, 2, 6], [0, 2, 7], [0, 2, 8], [0, 2, 9], [0, 3, 4], [0, 3, 5], [0, 3, 6], [0, 3, 7], [0, 3, 8], [0, 3, 9], [0, 4, 5], [0, 4, 6], [0, 4, 7], [0, 4, 8], [0, 4, 9], [0, 5, 6], [0, 5, 7], [0, 5, 8], [0, 5, 9], [0, 6, 7], [0, 6, 8], [0, 6, 9], [0, 7, 8], [0, 7, 9], [0, 8, 9], [1, 2, 3], [1, 2, 4], [1, 2, 5], [1, 2, 6], [1, 2, 7], [1, 2, 8], [1, 2, 9], [1, 3, 4], [1, 3, 5], [1, 3, 6], [1, 3, 7], [1, 3, 8], [1, 3, 9], [1, 4, 5], [1, 4, 6], [1, 4, 7], [1, 4, 8], [1, 4, 9], [1, 5, 6], [1, 5, 7], [1, 5, 8], [1, 5, 9], [1, 6, 7], [1, 6, 8], [1, 6, 9], [1, 7, 8], [1, 7, 9], [1, 8, 9], [2, 3, 4], [2, 3, 5], [2, 3, 6], [2, 3, 7], [2, 3, 8], [2, 3, 9], [2, 4, 5], [2, 4, 6], [2, 4, 7], [2, 4, 8], [2, 4, 9], [2, 5, 6], [2, 5, 7], [2, 5, 8], [2, 5, 9], [2, 6, 7], [2, 6, 8], [2, 6, 9], [2, 7, 8], [2, 7, 9], [2, 8, 9], [3, 4, 5], [3, 4, 6], [3, 4, 7], [3, 4, 8], [3, 4, 9], [3, 5, 6], [3, 5, 7], [3, 5, 8], [3, 5, 9], [3, 6, 7], [3, 6, 8], [3, 6, 9], [3, 7, 8], [3, 7, 9], [3, 8, 9], [4, 5, 6], [4, 5, 7], [4, 5, 8], [4, 5, 9], [4, 6, 7], [4, 6, 8], [4, 6, 9], [4, 7, 8], [4, 7, 9], [4, 8, 9], [5, 6, 7], [5, 6, 8], [5, 6, 9], [5, 7, 8], [5, 7, 9], [5, 8, 9], [6, 7, 8], [6, 7, 9], [6, 8, 9], [7, 8, 9], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 5], [0, 1, 2, 6], [0, 1, 2, 7], [0, 1, 2, 8], [0, 1, 2, 9], [0, 1, 3, 4], [0, 1, 3, 5], [0, 1, 3, 6], [0, 1, 3, 7], [0, 1, 3, 8], [0, 1, 3, 9], [0, 1, 4, 5], [0, 1, 4, 6], [0, 1, 4, 7], [0, 1, 4, 8], [0, 1, 4, 9], [0, 1, 5, 6], [0, 1, 5, 7], [0, 1, 5, 8], [0, 1, 5, 9], [0, 1, 6, 7], [0, 1, 6, 8], [0, 1, 6, 9], [0, 1, 7, 8], [0, 1, 7, 9], [0, 1, 8, 9], [0, 2, 3, 4], [0, 2, 3, 5], [0, 2, 3, 6], [0, 2, 3, 7], [0, 2, 3, 8], [0, 2, 3, 9], [0, 2, 4, 5], [0, 2, 4, 6], [0, 2, 4, 7], [0, 2, 4, 8], [0, 2, 4, 9], [0, 2, 5, 6], [0, 2, 5, 7], [0, 2, 5, 8], [0, 2, 5, 9], [0, 2, 6, 7], [0, 2, 6, 8], [0, 2, 6, 9], [0, 2, 7, 8], [0, 2, 7, 9], [0, 2, 8, 9], [0, 3, 4, 5], [0, 3, 4, 6], [0, 3, 4, 7], [0, 3, 4, 8], [0, 3, 4, 9], [0, 3, 5, 6], [0, 3, 5, 7], [0, 3, 5, 8], [0, 3, 5, 9], [0, 3, 6, 7], [0, 3, 6, 8], [0, 3, 6, 9], [0, 3, 7, 8], [0, 3, 7, 9], [0, 3, 8, 9], [0, 4, 5, 6], [0, 4, 5, 7], [0, 4, 5, 8], [0, 4, 5, 9], [0, 4, 6, 7], [0, 4, 6, 8], [0, 4, 6, 9], [0, 4, 7, 8], [0, 4, 7, 9], [0, 4, 8, 9], [0, 5, 6, 7], [0, 5, 6, 8], [0, 5, 6, 9], [0, 5, 7, 8], [0, 5, 7, 9], [0, 5, 8, 9], [0, 6, 7, 8], [0, 6, 7, 9], [0, 6, 8, 9], [0, 7, 8, 9], [1, 2, 3, 4], [1, 2, 3, 5], [1, 2, 3, 6], [1, 2, 3, 7], [1, 2, 3, 8], [1, 2, 3, 9], [1, 2, 4, 5], [1, 2, 4, 6], [1, 2, 4, 7], [1, 2, 4, 8], [1, 2, 4, 9], [1, 2, 5, 6], [1, 2, 5, 7], [1, 2, 5, 8], [1, 2, 5, 9], [1, 2, 6, 7], [1, 2, 6, 8], [1, 2, 6, 9], [1, 2, 7, 8], [1, 2, 7, 9], [1, 2, 8, 9], [1, 3, 4, 5], [1, 3, 4, 6], [1, 3, 4, 7], [1, 3, 4, 8], [1, 3, 4, 9], [1, 3, 5, 6], [1, 3, 5, 7], [1, 3, 5, 8], [1, 3, 5, 9], [1, 3, 6, 7], [1, 3, 6, 8], [1, 3, 6, 9], [1, 3, 7, 8], [1, 3, 7, 9], [1, 3, 8, 9], [1, 4, 5, 6], [1, 4, 5, 7], [1, 4, 5, 8], [1, 4, 5, 9], [1, 4, 6, 7], [1, 4, 6, 8], [1, 4, 6, 9], [1, 4, 7, 8], [1, 4, 7, 9], [1, 4, 8, 9], [1, 5, 6, 7], [1, 5, 6, 8], [1, 5, 6, 9], [1, 5, 7, 8], [1, 5, 7, 9], [1, 5, 8, 9], [1, 6, 7, 8], [1, 6, 7, 9], [1, 6, 8, 9], [1, 7, 8, 9], [2, 3, 4, 5], [2, 3, 4, 6], [2, 3, 4, 7], [2, 3, 4, 8], [2, 3, 4, 9], [2, 3, 5, 6], [2, 3, 5, 7], [2, 3, 5, 8], [2, 3, 5, 9], [2, 3, 6, 7], [2, 3, 6, 8], [2, 3, 6, 9], [2, 3, 7, 8], [2, 3, 7, 9], [2, 3, 8, 9], [2, 4, 5, 6], [2, 4, 5, 7], [2, 4, 5, 8], [2, 4, 5, 9], [2, 4, 6, 7], [2, 4, 6, 8], [2, 4, 6, 9], [2, 4, 7, 8], [2, 4, 7, 9], [2, 4, 8, 9], [2, 5, 6, 7], [2, 5, 6, 8], [2, 5, 6, 9], [2, 5, 7, 8], [2, 5, 7, 9], [2, 5, 8, 9], [2, 6, 7, 8], [2, 6, 7, 9], [2, 6, 8, 9], [2, 7, 8, 9], [3, 4, 5, 6], [3, 4, 5, 7], [3, 4, 5, 8], [3, 4, 5, 9], [3, 4, 6, 7], [3, 4, 6, 8], [3, 4, 6, 9], [3, 4, 7, 8], [3, 4, 7, 9], [3, 4, 8, 9], [3, 5, 6, 7], [3, 5, 6, 8], [3, 5, 6, 9], [3, 5, 7, 8], [3, 5, 7, 9], [3, 5, 8, 9], [3, 6, 7, 8], [3, 6, 7, 9], [3, 6, 8, 9], [3, 7, 8, 9], [4, 5, 6, 7], [4, 5, 6, 8], [4, 5, 6, 9], [4, 5, 7, 8], [4, 5, 7, 9], [4, 5, 8, 9], [4, 6, 7, 8], [4, 6, 7, 9], [4, 6, 8, 9], [4, 7, 8, 9], [5, 6, 7, 8], [5, 6, 7, 9], [5, 6, 8, 9], [5, 7, 8, 9], [6, 7, 8, 9], [0, 1, 2, 3, 4], [0, 1, 2, 3, 5], [0, 1, 2, 3, 6], [0, 1, 2, 3, 7], [0, 1, 2, 3, 8], [0, 1, 2, 3, 9], [0, 1, 2, 4, 5], [0, 1, 2, 4, 6], [0, 1, 2, 4, 7], [0, 1, 2, 4, 8], [0, 1, 2, 4, 9], [0, 1, 2, 5, 6], [0, 1, 2, 5, 7], [0, 1, 2, 5, 8], [0, 1, 2, 5, 9], [0, 1, 2, 6, 7], [0, 1, 2, 6, 8], [0, 1, 2, 6, 9], [0, 1, 2, 7, 8], [0, 1, 2, 7, 9], [0, 1, 2, 8, 9], [0, 1, 3, 4, 5], [0, 1, 3, 4, 6], [0, 1, 3, 4, 7], [0, 1, 3, 4, 8], [0, 1, 3, 4, 9], [0, 1, 3, 5, 6], [0, 1, 3, 5, 7], [0, 1, 3, 5, 8], [0, 1, 3, 5, 9], [0, 1, 3, 6, 7], [0, 1, 3, 6, 8], [0, 1, 3, 6, 9], [0, 1, 3, 7, 8], [0, 1, 3, 7, 9], [0, 1, 3, 8, 9], [0, 1, 4, 5, 6], [0, 1, 4, 5, 7], [0, 1, 4, 5, 8], [0, 1, 4, 5, 9], [0, 1, 4, 6, 7], [0, 1, 4, 6, 8], [0, 1, 4, 6, 9], [0, 1, 4, 7, 8], [0, 1, 4, 7, 9], [0, 1, 4, 8, 9], [0, 1, 5, 6, 7], [0, 1, 5, 6, 8], [0, 1, 5, 6, 9], [0, 1, 5, 7, 8], [0, 1, 5, 7, 9], [0, 1, 5, 8, 9], [0, 1, 6, 7, 8], [0, 1, 6, 7, 9], [0, 1, 6, 8, 9], [0, 1, 7, 8, 9], [0, 2, 3, 4, 5], [0, 2, 3, 4, 6], [0, 2, 3, 4, 7], [0, 2, 3, 4, 8], [0, 2, 3, 4, 9], [0, 2, 3, 5, 6], [0, 2, 3, 5, 7], [0, 2, 3, 5, 8], [0, 2, 3, 5, 9], [0, 2, 3, 6, 7], [0, 2, 3, 6, 8], [0, 2, 3, 6, 9], [0, 2, 3, 7, 8], [0, 2, 3, 7, 9], [0, 2, 3, 8, 9], [0, 2, 4, 5, 6], [0, 2, 4, 5, 7], [0, 2, 4, 5, 8], [0, 2, 4, 5, 9], [0, 2, 4, 6, 7], [0, 2, 4, 6, 8], [0, 2, 4, 6, 9], [0, 2, 4, 7, 8], [0, 2, 4, 7, 9], [0, 2, 4, 8, 9], [0, 2, 5, 6, 7], [0, 2, 5, 6, 8], [0, 2, 5, 6, 9], [0, 2, 5, 7, 8], [0, 2, 5, 7, 9], [0, 2, 5, 8, 9], [0, 2, 6, 7, 8], [0, 2, 6, 7, 9], [0, 2, 6, 8, 9], [0, 2, 7, 8, 9], [0, 3, 4, 5, 6], [0, 3, 4, 5, 7], [0, 3, 4, 5, 8], [0, 3, 4, 5, 9], [0, 3, 4, 6, 7], [0, 3, 4, 6, 8], [0, 3, 4, 6, 9], [0, 3, 4, 7, 8], [0, 3, 4, 7, 9], [0, 3, 4, 8, 9], [0, 3, 5, 6, 7], [0, 3, 5, 6, 8], [0, 3, 5, 6, 9], [0, 3, 5, 7, 8], [0, 3, 5, 7, 9], [0, 3, 5, 8, 9], [0, 3, 6, 7, 8], [0, 3, 6, 7, 9], [0, 3, 6, 8, 9], [0, 3, 7, 8, 9], [0, 4, 5, 6, 7], [0, 4, 5, 6, 8], [0, 4, 5, 6, 9], [0, 4, 5, 7, 8], [0, 4, 5, 7, 9], [0, 4, 5, 8, 9], [0, 4, 6, 7, 8], [0, 4, 6, 7, 9], [0, 4, 6, 8, 9], [0, 4, 7, 8, 9], [0, 5, 6, 7, 8], [0, 5, 6, 7, 9], [0, 5, 6, 8, 9], [0, 5, 7, 8, 9], [0, 6, 7, 8, 9], [1, 2, 3, 4, 5], [1, 2, 3, 4, 6], [1, 2, 3, 4, 7], [1, 2, 3, 4, 8], [1, 2, 3, 4, 9], [1, 2, 3, 5, 6], [1, 2, 3, 5, 7], [1, 2, 3, 5, 8], [1, 2, 3, 5, 9], [1, 2, 3, 6, 7], [1, 2, 3, 6, 8], [1, 2, 3, 6, 9], [1, 2, 3, 7, 8], [1, 2, 3, 7, 9], [1, 2, 3, 8, 9], [1, 2, 4, 5, 6], [1, 2, 4, 5, 7], [1, 2, 4, 5, 8], [1, 2, 4, 5, 9], [1, 2, 4, 6, 7], [1, 2, 4, 6, 8], [1, 2, 4, 6, 9], [1, 2, 4, 7, 8], [1, 2, 4, 7, 9], [1, 2, 4, 8, 9], [1, 2, 5, 6, 7], [1, 2, 5, 6, 8], [1, 2, 5, 6, 9], [1, 2, 5, 7, 8], [1, 2, 5, 7, 9], [1, 2, 5, 8, 9], [1, 2, 6, 7, 8], [1, 2, 6, 7, 9], [1, 2, 6, 8, 9], [1, 2, 7, 8, 9], [1, 3, 4, 5, 6], [1, 3, 4, 5, 7], [1, 3, 4, 5, 8], [1, 3, 4, 5, 9], [1, 3, 4, 6, 7], [1, 3, 4, 6, 8], [1, 3, 4, 6, 9], [1, 3, 4, 7, 8], [1, 3, 4, 7, 9], [1, 3, 4, 8, 9], [1, 3, 5, 6, 7], [1, 3, 5, 6, 8], [1, 3, 5, 6, 9], [1, 3, 5, 7, 8], [1, 3, 5, 7, 9], [1, 3, 5, 8, 9], [1, 3, 6, 7, 8], [1, 3, 6, 7, 9], [1, 3, 6, 8, 9], [1, 3, 7, 8, 9], [1, 4, 5, 6, 7], [1, 4, 5, 6, 8], [1, 4, 5, 6, 9], [1, 4, 5, 7, 8], [1, 4, 5, 7, 9], [1, 4, 5, 8, 9], [1, 4, 6, 7, 8], [1, 4, 6, 7, 9], [1, 4, 6, 8, 9], [1, 4, 7, 8, 9], [1, 5, 6, 7, 8], [1, 5, 6, 7, 9], [1, 5, 6, 8, 9], [1, 5, 7, 8, 9], [1, 6, 7, 8, 9], [2, 3, 4, 5, 6], [2, 3, 4, 5, 7], [2, 3, 4, 5, 8], [2, 3, 4, 5, 9], [2, 3, 4, 6, 7], [2, 3, 4, 6, 8], [2, 3, 4, 6, 9], [2, 3, 4, 7, 8], [2, 3, 4, 7, 9], [2, 3, 4, 8, 9], [2, 3, 5, 6, 7], [2, 3, 5, 6, 8], [2, 3, 5, 6, 9], [2, 3, 5, 7, 8], [2, 3, 5, 7, 9], [2, 3, 5, 8, 9], [2, 3, 6, 7, 8], [2, 3, 6, 7, 9], [2, 3, 6, 8, 9], [2, 3, 7, 8, 9], [2, 4, 5, 6, 7], [2, 4, 5, 6, 8], [2, 4, 5, 6, 9], [2, 4, 5, 7, 8], [2, 4, 5, 7, 9], [2, 4, 5, 8, 9], [2, 4, 6, 7, 8], [2, 4, 6, 7, 9], [2, 4, 6, 8, 9], [2, 4, 7, 8, 9], [2, 5, 6, 7, 8], [2, 5, 6, 7, 9], [2, 5, 6, 8, 9], [2, 5, 7, 8, 9], [2, 6, 7, 8, 9], [3, 4, 5, 6, 7], [3, 4, 5, 6, 8], [3, 4, 5, 6, 9], [3, 4, 5, 7, 8], [3, 4, 5, 7, 9], [3, 4, 5, 8, 9], [3, 4, 6, 7, 8], [3, 4, 6, 7, 9], [3, 4, 6, 8, 9], [3, 4, 7, 8, 9], [3, 5, 6, 7, 8], [3, 5, 6, 7, 9], [3, 5, 6, 8, 9], [3, 5, 7, 8, 9], [3, 6, 7, 8, 9], [4, 5, 6, 7, 8], [4, 5, 6, 7, 9], [4, 5, 6, 8, 9], [4, 5, 7, 8, 9], [4, 6, 7, 8, 9], [5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 6], [0, 1, 2, 3, 4, 7], [0, 1, 2, 3, 4, 8], [0, 1, 2, 3, 4, 9], [0, 1, 2, 3, 5, 6], [0, 1, 2, 3, 5, 7], [0, 1, 2, 3, 5, 8], [0, 1, 2, 3, 5, 9], [0, 1, 2, 3, 6, 7], [0, 1, 2, 3, 6, 8], [0, 1, 2, 3, 6, 9], [0, 1, 2, 3, 7, 8], [0, 1, 2, 3, 7, 9], [0, 1, 2, 3, 8, 9], [0, 1, 2, 4, 5, 6], [0, 1, 2, 4, 5, 7], [0, 1, 2, 4, 5, 8], [0, 1, 2, 4, 5, 9], [0, 1, 2, 4, 6, 7], [0, 1, 2, 4, 6, 8], [0, 1, 2, 4, 6, 9], [0, 1, 2, 4, 7, 8], [0, 1, 2, 4, 7, 9], [0, 1, 2, 4, 8, 9], [0, 1, 2, 5, 6, 7], [0, 1, 2, 5, 6, 8], [0, 1, 2, 5, 6, 9], [0, 1, 2, 5, 7, 8], [0, 1, 2, 5, 7, 9], [0, 1, 2, 5, 8, 9], [0, 1, 2, 6, 7, 8], [0, 1, 2, 6, 7, 9], [0, 1, 2, 6, 8, 9], [0, 1, 2, 7, 8, 9], [0, 1, 3, 4, 5, 6], [0, 1, 3, 4, 5, 7], [0, 1, 3, 4, 5, 8], [0, 1, 3, 4, 5, 9], [0, 1, 3, 4, 6, 7], [0, 1, 3, 4, 6, 8], [0, 1, 3, 4, 6, 9], [0, 1, 3, 4, 7, 8], [0, 1, 3, 4, 7, 9], [0, 1, 3, 4, 8, 9], [0, 1, 3, 5, 6, 7], [0, 1, 3, 5, 6, 8], [0, 1, 3, 5, 6, 9], [0, 1, 3, 5, 7, 8], [0, 1, 3, 5, 7, 9], [0, 1, 3, 5, 8, 9], [0, 1, 3, 6, 7, 8], [0, 1, 3, 6, 7, 9], [0, 1, 3, 6, 8, 9], [0, 1, 3, 7, 8, 9], [0, 1, 4, 5, 6, 7], [0, 1, 4, 5, 6, 8], [0, 1, 4, 5, 6, 9], [0, 1, 4, 5, 7, 8], [0, 1, 4, 5, 7, 9], [0, 1, 4, 5, 8, 9], [0, 1, 4, 6, 7, 8], [0, 1, 4, 6, 7, 9], [0, 1, 4, 6, 8, 9], [0, 1, 4, 7, 8, 9], [0, 1, 5, 6, 7, 8], [0, 1, 5, 6, 7, 9], [0, 1, 5, 6, 8, 9], [0, 1, 5, 7, 8, 9], [0, 1, 6, 7, 8, 9], [0, 2, 3, 4, 5, 6], [0, 2, 3, 4, 5, 7], [0, 2, 3, 4, 5, 8], [0, 2, 3, 4, 5, 9], [0, 2, 3, 4, 6, 7], [0, 2, 3, 4, 6, 8], [0, 2, 3, 4, 6, 9], [0, 2, 3, 4, 7, 8], [0, 2, 3, 4, 7, 9], [0, 2, 3, 4, 8, 9], [0, 2, 3, 5, 6, 7], [0, 2, 3, 5, 6, 8], [0, 2, 3, 5, 6, 9], [0, 2, 3, 5, 7, 8], [0, 2, 3, 5, 7, 9], [0, 2, 3, 5, 8, 9], [0, 2, 3, 6, 7, 8], [0, 2, 3, 6, 7, 9], [0, 2, 3, 6, 8, 9], [0, 2, 3, 7, 8, 9], [0, 2, 4, 5, 6, 7], [0, 2, 4, 5, 6, 8], [0, 2, 4, 5, 6, 9], [0, 2, 4, 5, 7, 8], [0, 2, 4, 5, 7, 9], [0, 2, 4, 5, 8, 9], [0, 2, 4, 6, 7, 8], [0, 2, 4, 6, 7, 9], [0, 2, 4, 6, 8, 9], [0, 2, 4, 7, 8, 9], [0, 2, 5, 6, 7, 8], [0, 2, 5, 6, 7, 9], [0, 2, 5, 6, 8, 9], [0, 2, 5, 7, 8, 9], [0, 2, 6, 7, 8, 9], [0, 3, 4, 5, 6, 7], [0, 3, 4, 5, 6, 8], [0, 3, 4, 5, 6, 9], [0, 3, 4, 5, 7, 8], [0, 3, 4, 5, 7, 9], [0, 3, 4, 5, 8, 9], [0, 3, 4, 6, 7, 8], [0, 3, 4, 6, 7, 9], [0, 3, 4, 6, 8, 9], [0, 3, 4, 7, 8, 9], [0, 3, 5, 6, 7, 8], [0, 3, 5, 6, 7, 9], [0, 3, 5, 6, 8, 9], [0, 3, 5, 7, 8, 9], [0, 3, 6, 7, 8, 9], [0, 4, 5, 6, 7, 8], [0, 4, 5, 6, 7, 9], [0, 4, 5, 6, 8, 9], [0, 4, 5, 7, 8, 9], [0, 4, 6, 7, 8, 9], [0, 5, 6, 7, 8, 9], [1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 7], [1, 2, 3, 4, 5, 8], [1, 2, 3, 4, 5, 9], [1, 2, 3, 4, 6, 7], [1, 2, 3, 4, 6, 8], [1, 2, 3, 4, 6, 9], [1, 2, 3, 4, 7, 8], [1, 2, 3, 4, 7, 9], [1, 2, 3, 4, 8, 9], [1, 2, 3, 5, 6, 7], [1, 2, 3, 5, 6, 8], [1, 2, 3, 5, 6, 9], [1, 2, 3, 5, 7, 8], [1, 2, 3, 5, 7, 9], [1, 2, 3, 5, 8, 9], [1, 2, 3, 6, 7, 8], [1, 2, 3, 6, 7, 9], [1, 2, 3, 6, 8, 9], [1, 2, 3, 7, 8, 9], [1, 2, 4, 5, 6, 7], [1, 2, 4, 5, 6, 8], [1, 2, 4, 5, 6, 9], [1, 2, 4, 5, 7, 8], [1, 2, 4, 5, 7, 9], [1, 2, 4, 5, 8, 9], [1, 2, 4, 6, 7, 8], [1, 2, 4, 6, 7, 9], [1, 2, 4, 6, 8, 9], [1, 2, 4, 7, 8, 9], [1, 2, 5, 6, 7, 8], [1, 2, 5, 6, 7, 9], [1, 2, 5, 6, 8, 9], [1, 2, 5, 7, 8, 9], [1, 2, 6, 7, 8, 9], [1, 3, 4, 5, 6, 7], [1, 3, 4, 5, 6, 8], [1, 3, 4, 5, 6, 9], [1, 3, 4, 5, 7, 8], [1, 3, 4, 5, 7, 9], [1, 3, 4, 5, 8, 9], [1, 3, 4, 6, 7, 8], [1, 3, 4, 6, 7, 9], [1, 3, 4, 6, 8, 9], [1, 3, 4, 7, 8, 9], [1, 3, 5, 6, 7, 8], [1, 3, 5, 6, 7, 9], [1, 3, 5, 6, 8, 9], [1, 3, 5, 7, 8, 9], [1, 3, 6, 7, 8, 9], [1, 4, 5, 6, 7, 8], [1, 4, 5, 6, 7, 9], [1, 4, 5, 6, 8, 9], [1, 4, 5, 7, 8, 9], [1, 4, 6, 7, 8, 9], [1, 5, 6, 7, 8, 9], [2, 3, 4, 5, 6, 7], [2, 3, 4, 5, 6, 8], [2, 3, 4, 5, 6, 9], [2, 3, 4, 5, 7, 8], [2, 3, 4, 5, 7, 9], [2, 3, 4, 5, 8, 9], [2, 3, 4, 6, 7, 8], [2, 3, 4, 6, 7, 9], [2, 3, 4, 6, 8, 9], [2, 3, 4, 7, 8, 9], [2, 3, 5, 6, 7, 8], [2, 3, 5, 6, 7, 9], [2, 3, 5, 6, 8, 9], [2, 3, 5, 7, 8, 9], [2, 3, 6, 7, 8, 9], [2, 4, 5, 6, 7, 8], [2, 4, 5, 6, 7, 9], [2, 4, 5, 6, 8, 9], [2, 4, 5, 7, 8, 9], [2, 4, 6, 7, 8, 9], [2, 5, 6, 7, 8, 9], [3, 4, 5, 6, 7, 8], [3, 4, 5, 6, 7, 9], [3, 4, 5, 6, 8, 9], [3, 4, 5, 7, 8, 9], [3, 4, 6, 7, 8, 9], [3, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6], [0, 1, 2, 3, 4, 5, 7], [0, 1, 2, 3, 4, 5, 8], [0, 1, 2, 3, 4, 5, 9], [0, 1, 2, 3, 4, 6, 7], [0, 1, 2, 3, 4, 6, 8], [0, 1, 2, 3, 4, 6, 9], [0, 1, 2, 3, 4, 7, 8], [0, 1, 2, 3, 4, 7, 9], [0, 1, 2, 3, 4, 8, 9], [0, 1, 2, 3, 5, 6, 7], [0, 1, 2, 3, 5, 6, 8], [0, 1, 2, 3, 5, 6, 9], [0, 1, 2, 3, 5, 7, 8], [0, 1, 2, 3, 5, 7, 9], [0, 1, 2, 3, 5, 8, 9], [0, 1, 2, 3, 6, 7, 8], [0, 1, 2, 3, 6, 7, 9], [0, 1, 2, 3, 6, 8, 9], [0, 1, 2, 3, 7, 8, 9], [0, 1, 2, 4, 5, 6, 7], [0, 1, 2, 4, 5, 6, 8], [0, 1, 2, 4, 5, 6, 9], [0, 1, 2, 4, 5, 7, 8], [0, 1, 2, 4, 5, 7, 9], [0, 1, 2, 4, 5, 8, 9], [0, 1, 2, 4, 6, 7, 8], [0, 1, 2, 4, 6, 7, 9], [0, 1, 2, 4, 6, 8, 9], [0, 1, 2, 4, 7, 8, 9], [0, 1, 2, 5, 6, 7, 8], [0, 1, 2, 5, 6, 7, 9], [0, 1, 2, 5, 6, 8, 9], [0, 1, 2, 5, 7, 8, 9], [0, 1, 2, 6, 7, 8, 9], [0, 1, 3, 4, 5, 6, 7], [0, 1, 3, 4, 5, 6, 8], [0, 1, 3, 4, 5, 6, 9], [0, 1, 3, 4, 5, 7, 8], [0, 1, 3, 4, 5, 7, 9], [0, 1, 3, 4, 5, 8, 9], [0, 1, 3, 4, 6, 7, 8], [0, 1, 3, 4, 6, 7, 9], [0, 1, 3, 4, 6, 8, 9], [0, 1, 3, 4, 7, 8, 9], [0, 1, 3, 5, 6, 7, 8], [0, 1, 3, 5, 6, 7, 9], [0, 1, 3, 5, 6, 8, 9], [0, 1, 3, 5, 7, 8, 9], [0, 1, 3, 6, 7, 8, 9], [0, 1, 4, 5, 6, 7, 8], [0, 1, 4, 5, 6, 7, 9], [0, 1, 4, 5, 6, 8, 9], [0, 1, 4, 5, 7, 8, 9], [0, 1, 4, 6, 7, 8, 9], [0, 1, 5, 6, 7, 8, 9], [0, 2, 3, 4, 5, 6, 7], [0, 2, 3, 4, 5, 6, 8], [0, 2, 3, 4, 5, 6, 9], [0, 2, 3, 4, 5, 7, 8], [0, 2, 3, 4, 5, 7, 9], [0, 2, 3, 4, 5, 8, 9], [0, 2, 3, 4, 6, 7, 8], [0, 2, 3, 4, 6, 7, 9], [0, 2, 3, 4, 6, 8, 9], [0, 2, 3, 4, 7, 8, 9], [0, 2, 3, 5, 6, 7, 8], [0, 2, 3, 5, 6, 7, 9], [0, 2, 3, 5, 6, 8, 9], [0, 2, 3, 5, 7, 8, 9], [0, 2, 3, 6, 7, 8, 9], [0, 2, 4, 5, 6, 7, 8], [0, 2, 4, 5, 6, 7, 9], [0, 2, 4, 5, 6, 8, 9], [0, 2, 4, 5, 7, 8, 9], [0, 2, 4, 6, 7, 8, 9], [0, 2, 5, 6, 7, 8, 9], [0, 3, 4, 5, 6, 7, 8], [0, 3, 4, 5, 6, 7, 9], [0, 3, 4, 5, 6, 8, 9], [0, 3, 4, 5, 7, 8, 9], [0, 3, 4, 6, 7, 8, 9], [0, 3, 5, 6, 7, 8, 9], [0, 4, 5, 6, 7, 8, 9], [1, 2, 3, 4, 5, 6, 7], [1, 2, 3, 4, 5, 6, 8], [1, 2, 3, 4, 5, 6, 9], [1, 2, 3, 4, 5, 7, 8], [1, 2, 3, 4, 5, 7, 9], [1, 2, 3, 4, 5, 8, 9], [1, 2, 3, 4, 6, 7, 8], [1, 2, 3, 4, 6, 7, 9], [1, 2, 3, 4, 6, 8, 9], [1, 2, 3, 4, 7, 8, 9], [1, 2, 3, 5, 6, 7, 8], [1, 2, 3, 5, 6, 7, 9], [1, 2, 3, 5, 6, 8, 9], [1, 2, 3, 5, 7, 8, 9], [1, 2, 3, 6, 7, 8, 9], [1, 2, 4, 5, 6, 7, 8], [1, 2, 4, 5, 6, 7, 9], [1, 2, 4, 5, 6, 8, 9], [1, 2, 4, 5, 7, 8, 9], [1, 2, 4, 6, 7, 8, 9], [1, 2, 5, 6, 7, 8, 9], [1, 3, 4, 5, 6, 7, 8], [1, 3, 4, 5, 6, 7, 9], [1, 3, 4, 5, 6, 8, 9], [1, 3, 4, 5, 7, 8, 9], [1, 3, 4, 6, 7, 8, 9], [1, 3, 5, 6, 7, 8, 9], [1, 4, 5, 6, 7, 8, 9], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 9], [2, 3, 4, 5, 6, 8, 9], [2, 3, 4, 5, 7, 8, 9], [2, 3, 4, 6, 7, 8, 9], [2, 3, 5, 6, 7, 8, 9], [2, 4, 5, 6, 7, 8, 9], [3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7], [0, 1, 2, 3, 4, 5, 6, 8], [0, 1, 2, 3, 4, 5, 6, 9], [0, 1, 2, 3, 4, 5, 7, 8], [0, 1, 2, 3, 4, 5, 7, 9], [0, 1, 2, 3, 4, 5, 8, 9], [0, 1, 2, 3, 4, 6, 7, 8], [0, 1, 2, 3, 4, 6, 7, 9], [0, 1, 2, 3, 4, 6, 8, 9], [0, 1, 2, 3, 4, 7, 8, 9], [0, 1, 2, 3, 5, 6, 7, 8], [0, 1, 2, 3, 5, 6, 7, 9], [0, 1, 2, 3, 5, 6, 8, 9], [0, 1, 2, 3, 5, 7, 8, 9], [0, 1, 2, 3, 6, 7, 8, 9], [0, 1, 2, 4, 5, 6, 7, 8], [0, 1, 2, 4, 5, 6, 7, 9], [0, 1, 2, 4, 5, 6, 8, 9], [0, 1, 2, 4, 5, 7, 8, 9], [0, 1, 2, 4, 6, 7, 8, 9], [0, 1, 2, 5, 6, 7, 8, 9], [0, 1, 3, 4, 5, 6, 7, 8], [0, 1, 3, 4, 5, 6, 7, 9], [0, 1, 3, 4, 5, 6, 8, 9], [0, 1, 3, 4, 5, 7, 8, 9], [0, 1, 3, 4, 6, 7, 8, 9], [0, 1, 3, 5, 6, 7, 8, 9], [0, 1, 4, 5, 6, 7, 8, 9], [0, 2, 3, 4, 5, 6, 7, 8], [0, 2, 3, 4, 5, 6, 7, 9], [0, 2, 3, 4, 5, 6, 8, 9], [0, 2, 3, 4, 5, 7, 8, 9], [0, 2, 3, 4, 6, 7, 8, 9], [0, 2, 3, 5, 6, 7, 8, 9], [0, 2, 4, 5, 6, 7, 8, 9], [0, 3, 4, 5, 6, 7, 8, 9], [1, 2, 3, 4, 5, 6, 7, 8], [1, 2, 3, 4, 5, 6, 7, 9], [1, 2, 3, 4, 5, 6, 8, 9], [1, 2, 3, 4, 5, 7, 8, 9], [1, 2, 3, 4, 6, 7, 8, 9], [1, 2, 3, 5, 6, 7, 8, 9], [1, 2, 4, 5, 6, 7, 8, 9], [1, 3, 4, 5, 6, 7, 8, 9], [2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8], [0, 1, 2, 3, 4, 5, 6, 7, 9], [0, 1, 2, 3, 4, 5, 6, 8, 9], [0, 1, 2, 3, 4, 5, 7, 8, 9], [0, 1, 2, 3, 4, 6, 7, 8, 9], [0, 1, 2, 3, 5, 6, 7, 8, 9], [0, 1, 2, 4, 5, 6, 7, 8, 9], [0, 1, 3, 4, 5, 6, 7, 8, 9], [0, 2, 3, 4, 5, 6, 7, 8, 9], [1, 2, 3, 4, 5, 6, 7, 8, 9]]
    

In the high-order knockout, iterations knocking out the target feature should have a higher change in accuracy. Since, in my sample data there is no high order interactions, this association should dwindle with higher order interactions e.g. a 3-5 couplet will have a higher value than a 3-5-7-8-9 pentuplet.

## Installation
------

I might build a wheel later, but for now just download the dependencies- Numpy, Keras, and Tensorflow and import the scripts.
